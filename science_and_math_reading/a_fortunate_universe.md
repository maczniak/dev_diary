# [A Fortunate Universe][homepage] by [Geraint F. Lewis][geraint_lewis], [Luke A. Barnes][luke_barnes] and Brian Schmidt, Cambridge University Press (2016)

[fine-tuning][fine_tuning_wikipedia] (in Wikipedia)

[homepage]: http://www.cambridge.org/kr/academic/subjects/physics/history-philosophy-and-foundations-physics/fortunate-universe-life-finely-tuned-cosmos
[geraint_lewis]: http://cosmic-horizons.blogspot.kr/
[luke_barnes]: https://letterstonature.wordpress.com/
[fine_tuning_wikipedia]: https://en.wikipedia.org/wiki/Fine-tuning

## 1. A Conversation on Fine-Tuning

Why, in the almost infinite sea of possibilities, was our Universe born with the
 conditions that allow life to arise? That is the subject of this book.
Clearly, we can consider an infinite variety of universes, each with a differing
 electron mass, and the core question of fine-tuning is what fraction of these
 could support complex life.<br>
Before continuing, there is a potential confusion with term *fine-tuning* that
 we should address. To a physicist, 'fine-tuning' implies that there is a
 sensitivity of an outcome to some input parameters or assumptions. Just like
 baking a cake, if an experiment produces some spectacular result only for a
 particular, precise set-up, the experiment is said to be *fine-tuned* with
 respect to the result. 'Find-tuning for life' is a type of physics fine-tuning,
 where the outcome is life.<br>
'Fine-tuning- is a metaphor, one that brings to mind an old radio set with dials
 that must be delicately set in order to listen to Norfolk Nights on Radio
 Norwich. This metaphor unfortunately involves a guiding hand that sets the
 dials, giving the impression that 'fine-tuned' means cleverly arranged or made
 for a purpose by a *fine-tuner*. Whether such a fine-tuner of our Universe
 exists or not, this is not the sense in which we use the term. 'Fine-tuning' is
 a technical term borrowed from physics, and refers to the contrat between a
 wide range of possibilities and a narrow range of a particular outcome or
 phenomenon.<br>
It supposes that a universe that is right for life exists because there are
 untold multitudes of universes with different properties. In the cosmic
 lottery, we got lucky.
* Question 1: What Is Life?
* Question 2: What Is the Anthropic Principle?
* Question 3: What Is Science?

This does not sink the theory, but it does leave the door open for an
 alternative theory to better explain the data. This is what the physicist means
 by 'fine-tuned'--*a suspiciously precise assumption*. (Precision is great in
 our data, but not in our assumptions.)<br>
Small bugbear here, but some people talk about Einstein's Theory of General
 Relativity, not his General Theory of Relativity. The former is incorrect, as
 it is the theory that is general, not the relativity.<br>
Nothing in Newton's or Einstein's theory tells us the value of *G*. We have to
 ask nature, measuring from experiment. Similar constants appear in all of the
 force laws, where they are called *coupling constants*.

## 2. I'm Only Human!

What happens in a universe in which the electron and quark masses are slightly
 different?<br>
The Δ<sup>++</sup> decay also conserves what is known as *baryon number*: the
 total number of quarks minus the number of antiquarks is the same before and
 after the decay. So the proton holds a very important title: it is the lightest
 particle made from three quarks. (If baryon number is not perfectly conserved,
 then protons will eventually decy. This has never been observed, and the
 calculated timescale for such decays is many, many order of magnitude longer
 than the current age of the Universe. Here, we treat protons as stable.) The
 neutron, by contrast, decays. If left on its own, it has about 15 minutes
 before it becomes a proton, an electron and antineutrino.<br>
bosons (wuth an integer value of spin) and fermions (with half-integer
 spins)<br>
Protons and neutrons, themselves fermions, can only be found on distinct energy
 levels with the nucleus, packed in accordance with the Pauli exclusion
 principle. This explains why, while isolated neutrons rapidly decay, a neutron
 *inside* an atomic nucleus is effectively stable. It cannot decay because no
 lower engery positions are available for the resultant proton.<br>
If the electron mass (1836.15 times lighter than the proton) were within a
 factor of a hundred of the proton mass, the quantum jiggling of elctrons would
 destroy the lattice. In short: no solids.<br>
["Person of the Year" Nomination for Higgs Boson Riddled with Errors][person_of_the_year_nomination_for_higgs_boson_riddled_with_errors]<br>
The mathematical language in which the Standard Model is written goes by the
 scientifically sexy name of *quantum field theory*.<br>
The particles of the Standard Model of particle physics get their mass by
 interacting with the *Higgs field*. Note that the Higgs mechanism gives mass to
 the fundamental particles only. In composite particles, such as protons and
 neutrons, the individual quark masses make up only a tiny fraction of the mass.
 The remainder is in the form of the energy that binds the quarks together. The
 field itself can vibrate, and these waves behave like particles. The particle
 in question is the *Higgs boson*.<br>
Here's where the fine-tuning headache begins. When you add the contributions
 from the vacuum all up, the mass of the Higgs boson we should measure would be
 infinite. There is a hard upper limit, the *Planck energy*. We don't have a
 quantum theory of gravity, so extrapolating past the Planck energy (or
 equivalently, Planck mass) is pointless. Getting closer, but still a long, long
 way from the observed value (by 10<sup>16</sup>). Particle physicists' favoured
 potential solution is known as *supersymmetry*. There is no observational
 evidence for any of the supersymmetric partners. This means the supersymmetric
 partners, if they exist, must be very massive, more massive than the energy
 available at the Large Hadron Collider, our biggest particle accelerator.

[person_of_the_year_nomination_for_higgs_boson_riddled_with_errors]: https://blogs.scientificamerican.com/observations/person-of-the-year-nomination-for-higgs-boson-riddled-with-errors/

## 3. Can You Feel the Force?

The electromagnetic force is 'carried' by the photon, γ. Similarly, gravity is
 thought to be carried by the as yet undiscovered graviton, G. The strong
 nuclear force is carried by the massless gluons, so named because they glue the
 nucleus together. Finally, the weak force is the most enigmatic of the four,
 carried by three massive particles: Z<sup>0</sup>, W<sup>+</sup> and
 W<sup>-</sup>.<br>
Feynman was struggling with a theory due to Paul Dirac, known as quantum
 electrodynamics (QED). QED is ambitious, bringing together quantum theory,
 relativity, the electromagnetic force and the electron. In QED, both
 electromagnetism (and its photon) and the electron are represented using
 fields. / Most physicists believe that we can extend this picture to include
 gravity, with the force of gravity conveyed by the *graviton*. However, refusal
 of gravity to play ball with the other forces of the Universe is a major
 stumbling block in modern physics, as we will see.<br>
The coupling constants tell us the probability that a force particle will be
 exchanged.<br>
Nature has furnished us with 92 natural elements, from hydrogen to uranium, but
 in the lab we have been able to artificially create heavier elements. Their
 very short lives are spent seething and wobbling, until the grip of the strong
 force is broken by the electromagnetic repulsion of the positively charged
 protons, ripping the nucleus into smaller pieces.<br>
Different types of radioactivity can be classified by the particle emitted. The
 simplest is perhaps gamma(γ)-decay, where an excited nucleus emits a
 high-energy photon, in much the same way that the electrons of an atom can emit
 photons as they jump between energy levels. Alpha(α)-decay occurs when a heavy
 element emits the nucleus of a helium atom (two protons and two neutrons),
 which in this case is called an α-particle. The particle can cheat: the rules
 of quantum mechanics allow for the possibility that the particle will *tunnel*
 to the outside without ever having had the energy to jump over the wall. the
 α-particle within a nucleus may try to escape its prison many trillions of
 times per second. Finally beta(β)-decay occurs when a nucleus spits out an
 electron (flipping neutrons into protons), or its antimatter cousin, the
 positron (flipping protons into neutrons). This from of radioactivity is a
 manifestation of the weak force. Neutron decay: an example of the weak force in
 action, with a neutron emiting a W<sup>-</sup> particle as it changes into a
 proton. The W<sup>-</sup> then decays into an electron and a neutrino.<br>
However, eleminating radioactivity has some surprising consequences.<br>
The Earth's surface is merely a thin crust enveloping a hot interior of rock and
 iron. Picture the skin on an apple--*that* thin. Without an internal source of
 heat, the centre of the Earth would have cooled and solidified long ago.
 However, energy is injected into the rocks through the continual radioactive
 decay of its elements, particularly isotopes of potassium, uranium and thorium.
 Today , the radioactive heating of the Earth is dominated by α-emission by
 long-lived uranium and thorium atoms, while in the past the dominant heating
 was through β-decay of potassium. Our planetary neighbours, Mercury, Venus and
 Mars, have virtually no magnetic field and so are blasted by the solar wind,
 boiling elements off their atmospheres.<br>
In fact, it isn't clear that there will be any matter in the weak-less universe
 at all, as the weak interaction appears to be crucial for creating more matter
 than antimatter. Without this asymmetry, almost all matter annihilates into
 photons in the very early universe.

## 4. Energy and Entropy

There are four fundamental laws of thermodynamics, but for historial reasons
 they are numbered zero to three. The zeroth law allows us to define the concept
 of *temperature*. If two objects have the same temperature, then heat will not
 flow between them; they are in *thermal equilibrium*. The first law says that
 energy (including heat) is conserved, that is, neither created nor destroyed.
 The third law states that *absolute zero*, the temperature where all thermal
 motion *theoretically* ceases, cannot be reaced in practice.<br>
We can simplify things if we think of entropy in terms of *useful* or *free
 energy*. A system with low entropy has energy that can be extracted and
 converted into another form. By contrast, the energy of high-entropy systems is
 stuck, unable to be tapped.<br>
Order is at best maintained, and only created at the expense of disorder
 elsewhere. The second law of thermodynamics states that in any situation where
 energy flows, the total entropy never decreases. No energy, no life. But energy
 alone is not enough to create electricity or motion or metabolism. We need
 *useful* energy. Life needs low entropy.<br>
Energy is released as very high frequency photons, known as gamma rays. It takes
 hundreds of thousands of years to cover a distance that light could cover in a
 few seconds, if only it could travel in a straight line. Their bouncing path
 through the Sun has important consequences. As well as supporting the gas
 against gravity, it has dialled down the energy of the photons from sterilizing
 gamma rays to plant-fuelling optical photons.<br>
Our observations confirm these limits: nothing with fewer than 10<sup>56</sup>
 particles shines, and no star with more than 3 × 10<sup>59</sup> particles has
 been found. That's the *window*.<br>
The gravitational coupling constant depends on the proton mass. When considering
 astrophysical objects such as stars, we will speak in terms of making gravity
 stronger or weaker because it's more intuitive.<br>
If we nudge the strength of the strong force upwards by just 0.4 per cent, stars
 produce a wealth of carbon, but the route to oxygen is cut off. Decreasing the
 strength of the strong force by a similar 0.4 per cent has the opposite effect.
 Stars burn hydrogen at a leisurely rate, typically taking 10 billion years to
 exhaust their supply. But if large enough to ignite further reactions, the star
 will burn through helium in two million years, carbon in two thousand years,
 neon in a few months, and silicon in a few weeks. This increasingly rapid
 sequence, running out of one fuel and igniting another, produces complicated
 changes in the star, creating headaches for astronomers trying to understand
 the production of chemicals in the Universe. Calculating their combined
 influence in carbon, which has only six protons and six neutrons, might seem to
 be simple but, due to the complex nature of the strong nuclear force, it in
 fact requires supercomputer simulations. These calculational hurdles were
 overcome by physicists only on 2012. A team of researchers working at German
 and US institutes published the results of a calculation almost 10 million
 computational hours in the making. ... all constructed from molecules of carbon
 and oxygen, is only possible because the masses of the quarks and the strength
 of the forces lie within an outrageously narrow range!

## 5. The Universe Is Expanding

It is important to remember that dark matter isn't something weird 'out there',
 but permeates the Solar System, and even the room in which you are sitting, the
 train on which you are riding or even the tree under which you are lounging. It
 is only because of the large mass of the Sun and Earth in your vicinity that
 this dark matter goes unnoticed. If all the dark matter instantaneously
 disappeared, the Sun would exit the Galaxy at high speed, destined to end its
 days in the darkness of intergalactic space.<br>
Peeking at general relativity's cogs and springs, which turn energy into
 spacetime geometry, we find a system of 10 coupled, non-linear, partial
 differential equations. So cosmologists in the 1920s and '30s--the early days
 of relativity--did what any good physicist does: they oversimplified. They
 assumed that on sufficiently large 'cosmological' scales, the Universe is the
 same everywhere (*homogeneity*) and looks the same in all directions
 (*isotropy*). The result was the *Friedmann-Lemaître-Robertson-Walker (FLRW)
 model*. During the last hundred years of modern cosmology, various
 complications to the FLRW model have been investigated; non have improved on
 the original. The Universe is just about as simple as we could have hoped.
 Virtually every observation we have ever made of the Universe is explainable
 within this mathematical framework, a framework that can be fully written down
 in a page or two. The FLRW model describes two attributes of the spacetime of
 the Universe as a whole. The first is the *geometry* of space. Secondly, the
 FLRW model describes the *scale* of space.<br>
General relativity is a geometric theory, so it can tell us about the *geometry*
 of spacetime, but not the *topology*. That is, General Relativity can tell us
 how spacetime is locally curved, but not how it is globally connected.<br>
Think of *dark matter* as doesn't-send-us-any-light matter. Or, perhaps,
 we're-in-the-dark-matter. All the particles that you're ever seen, and that
 astronomers have ever seen through their telescopes, and that we've created in
 particle accelerators, cause the expansion of the Universe to slow down. The
 expansion of the Universe is speeding up. The stage is moving in ways that no
 *known actor* can cause. We need another casting call. We called this
 unfamiliar something *dark energy*. We're-even-more-in-the-dark energy. It's a
 convenient shorthand for 'whatever makes the Universe's expansion
 accelerate'.<br>
69% of our Universe is dark energy. 26% of our Universe is dark matter. 5% of
 our Universe is familiar, oridinary matter. 0.3% of our Universe is in stars
 (that is, 6% of the ordinary matter).
The Big Bang theory describes how the Universe was *raised*; how it was
 *born*--even *if* it was born--is another matter. The Big Bang theory is doing
 a marvellous job of explaining the Universe we see through our telescopes.<br>
[Galaxy Dynamics | John Dubinski][galaxy_dynamics_john_dubinski]<br>
This is the *cosmological constant problem*, though the name is a slight
 misnomer. The *effective* cosmological constant is the sum of Einstein's
 cosmological constant and all the forms of energy in a universe that behave
 like a cosmological constant. The accelerated expansion of our Universe is due
 to the effective cosmological constant. So the problem is this: why is the
 effective cosmological constant so much smaller than each of the vacuum
 energies?<br>
Suppose we wind the tape of the Universe back to when the first elements are
 formed, a few minutes after the Big Bang. Then, in order for our Universe to be
 flat to within one per cent today, it must have been flat to within one part in
 a thousand trillion (1 followed by 15 zeros). Suspicious! Winding the clock
 further back in time only makes it worse. The furthest we can meaningfully wind
 the clock back is the so-called *Planck time*, before which we would need a
 theory of quantum gravity to predict what's going on. The Planck time is equal
 to 10<sup>-44</sup> seconds. At this time, the fine-tuning is one part in
 10<sup>55</sup>. Even more suspicious! This is known as the *flatness
 problem*.<br>
These patches of gas have the same temperature to one part in 100,000. This is
 known as the *horizon problem*. The value of *Q*, the initial lumpiness in the
 Universe, needs to be tuned to a high degree to ensure an reasonably sedate
 beginning to the formation of structure, but not so sedate that it does not
 occur at all.<br>
This suggests a solution: a period of accelerating expansion in the very early
 Universe. This idea, which was developed by Alan Guth, Andrei Linde, Alexi
 Starobinsky, Paul Steinhardt and Andreas Albrecht in the late 1970s and early
 1980s, is known as *consmic inflation*. To solve these problems, inflation must
 have been extreme. Beginning roughly 10<sup>-35</sup> seconds after the initial
 birth of the Universe, inflation lasts until 10<sup>-34</sup> seconds. In that
 time, the Universe doubles in scale at least 80 times (about a trillion
 trillion times!). Thes problem with the theory of cosmic inflation is ... it's
 not really a theory. There's no physics. What causes inflation? The idea itself
 doesn't say. It's probably some form of field, and since fields are named after
 their associated particles, it's usually called the *inflation field*; this
 impressive sounding object is just another we're-in-the-dark name.<br>
In fact, almost 100,000,000,000 solar neutrinos stream through each square
 centimetre of your body every second. What happens to the electron-neutrinos is
 called *quantum mixing*. The mixing occurs all along the journey's path, and
 while the Sun creates 100 per cent electron-neutrinos, by the time they reach
 the Earth, only 33 per cent of them still are that type, with the other 67 per
 cent invisible to the detectors that were built to register the passage of
 electron-neutrinos. The mixing of neutrinos reveals a very important fact about
 them: neutrinos have mass. In fact, if we add up all of the various sources of
 neutrinos in the Universe, there are, on average, about 340 million per cubic
 metre. This completely swamps the average density of atoms in the Universe,
 which is only about two hydrogen atoms per cubic metre, and is roughly
 equivalent to the density of leftover radiation in the cosmic microwave
 background. The sum of the masses of the three types of neutrino is less than a
 millionth of the mass of the electron.<br>
But here, at about 10<sup>-43</sup> seconds, we eventually run out of
 speculation. Our best theory of gravity--Einstein's General Relativity--breaks
 down. Finally, at *t* = 0, the creation of the Universe, we hit a mathematical
 absurdity called a *singularity*. If we ask Einstein's theory, what was the
 density and temperature at the birth of the Universe, we get the answer
 'infinity'. It is important to remember that a singularity is not an object,
 and especially not a magical one; it is simply a mistake resulting from pushing
 our theories too far. In fact, in the late 1960s, Stephen Hawking and Roger
 Penrose showed that in all sorts of reasonable universes, a singularity at the
 beginning is unavoidable.

[galaxy_dynamics_john_dubinski]: http://www.cita.utoronto.ca/~dubinski/galaxydynamics/

## 6. All Bets Are Off!

However, that is *not* the point of the thought experiment. In the mathematical
 language of quantum mechanics, the answer to the question 'has the atom
 decayed?' is '*atom intact* + *atom decayed*'. Such a state is called a
 *superposition*. What we should *not* say is that 'the atom *both* has decayed
 and is intact'. In quantum mechanical language, there is a perfectly precise
 way to say 'the atom has decayed', and a way to say 'the atom is intact', and
 the superposition is *neither* of these. Rather, if we think that the wave
 function represents the system itself, we should say that it is the case that
 the atom has neither decayed nor is intact. (In quantum mechanics, 'it is not
 the case that the atom has decayed' does not imply that 'it is the case that
 the atom is intact'. The superposition 'decayed + intact' is something else
 entirely.) On the other hand, if we thin that the wave function represents our
 knowledge of the system, then we say that we don't know whether the atom has
 decayed or is intact. When we take the detector into account, quantum mechanics
 tells us that the entire 'atom and detector' system is described mathematically
 as 'decayed & detector triggered + intact & detector untriggered'. The wave
 function is still undecided. But, as the atom and detector interact with their
 surroundings, more things become *entangled* with the atom's indecision,
 including the cat. The system is soon described as 'decayed & detector
 triggered & cat dead + intact & detector untriggered & cat alive'. The atom's
 indecision is now cat sized! The point of this thought experiment is not that
 we must ponder how a cat can be both alive and dead, but instead to note that
 we never observe such a thing. We do not ourselves become a blurry blend of 'I
 see a living cat + I see a dead cat'. In quantum mechanics, we are presented
 with superposed, entangled alternatives. But the classic world--the world we
 observe--presents us with a single reality, with alternatives that are merely
 possible, not superposed.<br>
Here's where *decoherence* comes in. All the quantum weirdnes and waviness is
 only seen when we can keep track of the whole entangled system. This is much
 easier to do if the system is small. But, if the system has interacted with its
 environment then the quantum weirdness goes away. More precisely, it is because
 our measuring devices (such as he detector, the cat and your eye when you open
 the box) don't keep track of all the quantum entanglements that we fail to see
 quantum weirdness in large systems. Quantum subsystems, by interacting
 continuously and irreversibly with their environment, are forced into those
 special states that act classically. Decoherence destroys superpositions. Our
 increased understanding of decoherence has meant that scientists have learned
 how to amplify quantum weirdness without destroying it. We have been able to
 crete and observe large molecules, with 800 individual atoms, in superposed
 states. These molecules are approaching the size of the smallest life
 forms.<br>
A quantum calculation ends with a 'measurement'. That is, quantum computers need
 decoherence (at least) to finish the calculation.<br>
A particularly thorny problem in a quantum universe is *non-locality*. They can
 be *entangled*, and so in principle the ball's flight could be affected by
 anything else in the universe. To us, with our finite knowledge of the world
 around us, the ball would be radically unpredictable.

To many physists, the search for ultimate laws of physics simply is a search for
 the deepest symmetry. (see "Dreams of a Final Theory" by Steven Weinberg.) And,
 importantly, from symmetry we get *conservation laws*. For example, a symmetry
 hidden within the quantum mechanical wave function results in the conservation
 of electric charge. Why does this symmetry exist? In truth, we don't know.<br>
The fact that the Universe is expanding means that the spacetime *itself* does
 not possess time symmetry. So ... wait for it ... the Universe as a whole does
 not conserve energy! We can see this non-conservation in action. Light, for
 example, is redshifted to longer wavelengths as it travels through as expanding
 universe. Each photon loses energy, but where did it go? Answer: it doesn't go
 anywhere. Dark energy also refuses to conserve energy; if the Universe doubles
 in volume, then it contains double the amount of dark energy. This doesn't mean
 that chaos reigns. The energy of the Universe changes predictably, so we can
 follow how it flows and transforms.<br>
Why is the weak force asymmetric, when all of other forces are symmetric? It
 comes down to that elusive particle, the neutrino. If you examined the spin of
 neutrinos travelling towards you, you would find them all spinning in the
 clockwise direction.<br>
Why does our Universe allows a mild amount of baryon asymmetry, large enough to
 produce the matter that makes you and me, but small enough not to dissolve our
 protons? Why does our Universe *not* allow similar asymmetry with regards to
 charge conservation, the existence of which would lead to such chaos?

Consider a video camera pointed at the dark star. A recording of the light
 'from' the star would be steadily erased from the tape, converted into light
 and sent out of the lens of the camera. There is nothing in our laws of physics
 that prevents this. For the laws to account for our experience of the Universe,
 we must understand why the mathematical possibility of light from the future
 doesn't actually happen in our Universe. Within the Standard Model of particle
 physics, an electron sent backwards in time looks like its antimatter sibling,
 the positron! In fact, all antimatter particles can be viewed as their matter
 twins travelling backwards in time (*time-reversal symmetry*). Just as the weak
 force can break parity symmetry, it can break time-reversal symmetry, but very
 rarely and weakly. This very slight asymmetry in our laws does not explain why
 the state of our Universe is so profoundly asymmetric with respect to time.<br>
The thermodynamic arow of time points in the direction of increasing entropy.
 Important clues come from thinking about computers, because running a computer
 program increases entropy. In particular, storing information in the memory of
 a computer is *irreversible* since the computer overwrites and erases
 information. (This result is related to *Landauer's principle*. Any physical
 implementation of the program must create an increase in entropy. Simply put,
 computers get hot because they have to forget.) This is good reason to believe
 that any information-processing life form will experience the thermodynamic
 arrow of time, always remembering the thermodynamic past but not the
 future.<br>
This suggests a way out of our conundrum, a way known as the *past hypothesis*.
 We simply postulate that the Universe began in a low-entropy state, and that
 all our reasoning about what states are likely and unlikely must start this
 assumption. Instead of finding the arrow of time in the laws of nature
 themselves, we find it in the Universe's very special initial state.<br>
In General Relativity, it is only with three or more space dimensions that
 gravitational fields can exist in empty space. What about extra dimensions of
 time? This sounds like science fiction, but mathematically it's easy to add a
 couple of extra time dimensions into physics.

Fermat's 'principle of least time' shows its real power when it is applied not
 only to light's reflections, but to all of light's journeys. When light travels
 from air into water, its path is bent. *Snell's law* describes this effect.
 Since Fermat, this notion of the 'path of least time' has become central, not
 only to the motion of light, but all of physics. When we peer into the
 mathematical framework of the pillars of modern science, be it
 electromagnetism, quantum mechanics or Einstein's General Theory of Relativity,
 we see a more general but similar 'principle of least *action*' at work. In
 this case, *action* refers to a mathematical quantity that succinctly but
 powerfully encodes the laws of nature. (Figure 37, The action equation and the
 Euler-Lagrange equation. These are truly the central equations of modern
 physics, appearing in both relativity and quantum mechanics, and our attempts
 to breech the chasm between them.) Intriguingly, beneath the very different
 ideas about the universe found in Newtonian physics, Maxwell's
 electromagnetism, quantum mechanics and Einstein's relativity, we find a common
 core. At the most fundamental level, the physical rules that govern the
 workings of the Universe appear to be built upon the same mathematical
 foundation.<br>
Importantly, and rather astonishingly, within the Game of Life you can build a
 *universal Turing machine*. [conwaylife.com][conwaylife_com]<br>
Even in the utterly foreign realm of cellular automata, with discrete 'on-off'
 cells on a grid instead of fields and particles in space and time, we hear a
 familiar story. Interesting, complex, organized, information-processing
 universes require fine-tuned rules.<br>
Mathematicians define the *Kolmogorov-Chaitin complexity* of a string of
 characters to be the length of the shortest set of instructions that produces
 it. It is a mathematical theorem of Kolmogorov-Chaitin complexity that most
 strings are random. It is thus all the more remarkable that the strings of
 numbers that represent the results of our experiments and observations of the
 Universe can be summarized in just a few short equations. Our world can be
 encoded or summarized to an extraordinary degree. The Universe is
 comprehensible because it is compressible. "An Introduction to Kolmogorov
 Complexity and Its Applications" (Springer, 2008)<br>
In Newtonian gravity, there are two different concepts that we call *mass*.
 There is *inertial* mass, which measures how hard something is to push. Also,
 there is *gravitational* mass, which measures how hard something is to lift.
 Conceptually, inertial mass has no necessary connection with gravitational
 mass. And yet, to explain cases like the hammer and the feather, Newtonian
 gravity must assume--for no deeper reason--that for every object in the
 Universe, inertial mass and gravitational mass are exactly equal.

[conwaylife_com]: http://conwaylife.com/

## 7. A Dozen (or So) Reaction to Fine-Tuning

Most of interstellar space is hot, with temperatures between 6,000°C and 10,000
 , except for dense molecular clouds at -260 °C.<br>
In 1978, Michael Berry showed that in order to predict the next 56 collisions of
 a molecule of oxygen in this room, one must know the position of every particle
 in the observable Universe! Ignoring the miniscule gravitational pull of a
 single electron 10 billion light years away generates errors that render the
 prediction useless in about 10 nanoseconds.<br>
The distinction between the two forms of Bayesianism is somewhat slippery, but
 goes something like this. The subjective Bayesian aims to quantify the
 intensity of a particulr person's beliefs. The objective Bayesian, on the other
 hand, is trying to discover the rules that govern rationality. Edwin Jaynes,
 whose posthumous textbook *Probability Theory* is fast becoming the bible of
 probability in the physical sciences, imagined that we are programming a
 rational robot. We feed the robot certain information ('I just flipped a fair
 coin'), and then ask it about a statement ('The coin turned up heads'). The
 robot's task is to assign probabilities in light of *only* the information it
 is given. Objective Bayesians believe that probabilities can be tightly (if not
 uniquely) constrained by principles of reason, and aim to develop methods that
 overcome personal bias and prejudice. Objective Bayesianism is coming to
 dominate statistical practice in physics because of its elegance, its
 principled foundations, its transparent connection between principle and
 practice, and its clarity and unity of method. / One of the core principles of
 Bayesian probability theory is that information must not be arbitrarily
 ignored. All evidence must be taken into account.

## 8. A Conversation Continued

The laws of nature as we know them get more *symmetric* at higher energy. But as
 the Universe expanded and cooled, it reached temperatures where the laws of
 nature could not maintai this perfect symmetry. The symmetry is *broken*, and
 it is only when this happens that the 'constant' of nature (as we know them)
 take on a particular value. The same kind of process explains the formation of
 snowflakes. In warmer air, water exists as vapour and droplets, all identical.
 As the temperature drops, individual snowflakes begin to form as the water
 changes *phase* from gas to solid. While the physical process is the same for
 each of them, tiny differences in the starting pattern, such as a piece of dust
 here or a shard of ice there, means that each snowflake is unique. Our
 equations are merely approximations to some deeper, grander equations in which
 the 'constants' are dynamical entities such as fields.<br>
[Philosophy of Cosmology][philosophy_of_cosmology] video<br>
Boltzmann had a radical idea. He was amongst the first to realize that the
 second law is a *statistical* law, and to appreciate what this means: it isn't
 certain, but merely overwhelmingly likely, that the entropy of a closed system
 will increase with time. Boltzmann thought that perhaps, despite appearances,
 the universe as whole *is* in thermal equilibrium, resembling the distant
 future we expect for the part of the Universe that we see around us. But if we
 wait long enough, a region of the Universe will fluctuate into a low-entropy
 state. The overall universe could have existed indefinitely--or even
 infinitely--before the fluctuation that became our Universe. The chances of
 spontaneously forming something small, like our teapot, are much, much lighter
 than the chance of spontaneously forming a large patch of liveable universe. As
 Arthur Eddington pointed out in the 1930s, it is extremely unlikely in
 Boltmann's multiverse that we would see a universe like this one, with
 low-entropy pastures as far as the eye, and the telescope, can see. It is much
 more likely for a fluctuation to produce a fully formed brain, complete with
 memories of a body and life that never happened, than it is to fluctuate into a
 huge, life-bearing universe. In the long, dark, empty future of expanding
 universes, Boltzmann Brains come to vastly outnumber the various life forms
 that existed when the universe was middle aged and filled with stars and
 planets. This is what philosophers call the 'brain in a vat' thought
 experiment: how do I know that I'm not in the Matrix? But if I'm a Boltzmann
 Brain, then my experiences are not of a real universe. The ultimate test of a
 scientific theory is in predicting observations.<br>
There is so much that we haven't discussed. There are other theories of quantum
 gravity, such as *loop quantum gravity*, and even inflation has competitors,
 such as Robert Brandenberger's 'string gas cosmology'.<br>
*Modal logic* is the extension of basic 'true or false' logic to include these
 different types or *modes* of truth. In the vernacular, '2 + 2 = 4' is
 necessarily true, while 'my name is Luke' is only contingently true.<br>
Richard Dawkins presents this objection in Chapter 4 of *The God Delusion*. His
 version is a rough sketch, and so we have to fill in a few details; I *think*
 that the argument is supposed to go like this. (a) Organized, complex things
 are improbable. (b) A designer is more organized and more complex than the
 thing designed. (c) Thus, a designer is more improbable than the thing
 designed. (d) Thus, it is extremely improbable that God (the ultimate designer)
 exists. It is more likely that the Universe just exists.<br>
Theism's rival is naturalism, not science, and theism offers an explanation
 where naturalism offers *none*. If naturalism turns out to be more like than
 theism, it will be because theism unwisely tried to explain the unexplainable,
 not because naturalism offered a better, or indeed any, explanation of the
 ultimate laws of nature. So, the success of science looks the same on
 naturalism and theism.

[Leonard Susskind - All Stanford physics lectures in order][susskind_lectures]

[philosophy_of_cosmology]: https://www.youtube.com/watch?v=OtxZ_wLb_84
[susskind_lectures]: https://www.youtube.com/playlist?list=PLQrxduI9Pds1fm91Dmn8x1lo-O_kpZGk8

